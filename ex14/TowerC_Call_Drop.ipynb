{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Install the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watson OpenScale Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ibm-ai-openscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watson Machine Learning Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ibm-watson-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the Notebook after Installing the required packages. By clicking on `Kernel>Restart`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Dataset\n",
    "\n",
    "Select the `Insert Pandas Dataframe` option, after selecting the below cell. Ensure the variable name is `df_data_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Call Drop Model using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, metrics\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_data_1.drop(['CallDrop_prec'], axis=1)\n",
    "y=df_data_1.loc[:, 'CallDrop_prec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add a categorical transformer to your model pipeline. \n",
    "    You will need to add a label encoder into the model pipeline before storing it into WML '''\n",
    "\n",
    "categorical_features = [\"Start_Time_MM_DD_YYYY\", \"Traffic\", \"Weather\", \"Start_Time_HH_MM_SS_s\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', svm.SVC(kernel='linear'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store, Deploy and Score your Custom WML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "api_key = 'vGrtX_WAm0mDnd5ECi9a7_pxRQV551Dzrcwt_Hqz8cj1' #insert your cloud API key here\n",
    "location = 'us-south'\n",
    "\n",
    "WML_CREDENTIALS = {\n",
    " \"apikey\": api_key,\n",
    " \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n",
    "}\n",
    "\n",
    "ml_client = APIClient(WML_CREDENTIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space_uid= '91e4ef1a-4613-446f-8055-28e94e4c1962'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ml_client.set.default_space(space_uid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the model on WML\n",
    "sw_spec_id = ml_client.software_specifications.get_id_by_name('runtime-22.1-py3.9')\n",
    "print(\"Software Specification ID: {}\".format(sw_spec_id))\n",
    "\n",
    "meta_props={\n",
    " ml_client.repository.ModelMetaNames.NAME: 'Tower C',\n",
    " ml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_id,\n",
    " ml_client.repository.ModelMetaNames.TYPE: \"scikit-learn_1.0\",\n",
    "}\n",
    "\n",
    "\n",
    "published_model = ml_client.repository.store_model(pipeline,\n",
    "                                             meta_props=meta_props,\n",
    "                                             training_data=X_train,\n",
    "                                             training_target=y_train\n",
    "                                             )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "published_model_uid = ml_client.repository.get_model_id(published_model)\n",
    "\n",
    "\n",
    "ml_client.repository.list_models()\n",
    "published_model_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Deployment for your stored model\n",
    "deploy_meta = {\n",
    "     ml_client.deployments.ConfigurationMetaNames.NAME: 'Tower C Deployment',\n",
    "     ml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    " }\n",
    "created_deployment = ml_client.deployments.create(published_model_uid, meta_props=deploy_meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scoring_endpoint =  ml_client.deployments.get_scoring_href(created_deployment)\n",
    "deployment_uid=created_deployment['metadata']['id']\n",
    "deployment_uid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup your Watson Openscale Dashboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create the Watson Openscale Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator,BearerTokenAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "iam_authenticator = IAMAuthenticator(apikey=api_key)\n",
    "ai_client = APIClient(authenticator=iam_authenticator)\n",
    "ai_client.data_marts.show()\n",
    "\n",
    "ai_client.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_mart_id = '828a6f4f-4b9d-47ec-ba42-258493a61e97'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Setup the Datamart on AI OpenScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mart_details = ai_client.data_marts.get(data_mart_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Add your Machine Learning Provider\n",
    "\n",
    "Watson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model.\n",
    "\n",
    "Note: You can bind more than one engine instance if needed by calling wos_client.service_providers.add method. Next, you can refer to particular service provider using service_provider_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ai_client.service_providers.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SERVICE_PROVIDER_NAME = \"Watson Machine Learning - CallDrop\"\n",
    "SERVICE_PROVIDER_DESCRIPTION = \"Added by Telecom Call Drop notebook.\"\n",
    "added_service_provider_result = ai_client.service_providers.add(\n",
    "        name=SERVICE_PROVIDER_NAME,\n",
    "        description=SERVICE_PROVIDER_DESCRIPTION,\n",
    "        service_type=ServiceTypes.WATSON_MACHINE_LEARNING,\n",
    "        deployment_space_id = space_uid,\n",
    "        operational_space_id = \"production\",\n",
    "        credentials=WMLCredentialsCloud(\n",
    "            apikey=api_key,      ## use `apikey=IAM_TOKEN` if using IAM_TOKEN to initiate client\n",
    "            url=WML_CREDENTIALS[\"url\"],\n",
    "            instance_id=None\n",
    "        ),\n",
    "        background_mode=False\n",
    "    ).result\n",
    "service_provider_id = added_service_provider_result.metadata.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client.service_providers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_deployment_details = ai_client.service_providers.list_assets(data_mart_id=data_mart_id, service_provider_id=service_provider_id, deployment_space_id = space_uid).result['resources'][0]\n",
    "asset_deployment_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_asset_details_from_deployment=ai_client.service_providers.get_deployment_asset(data_mart_id=data_mart_id,service_provider_id=service_provider_id,deployment_id=deployment_uid,deployment_space_id=space_uid)\n",
    "model_asset_details_from_deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Perform Initial Scoring for your Model Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=X_test.tail(20)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\n",
    "scoring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=list(X_test.columns)\n",
    "print(len(fields))\n",
    "fields, scoring_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_payload = {\n",
    "ml_client.deployments.ScoringMetaNames.INPUT_DATA: [{\n",
    " 'values': scoring_data\n",
    "}]\n",
    "}\n",
    "print(job_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_response = ml_client.deployments.score(deployment_uid, job_payload)\n",
    "\n",
    "print(scoring_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Create a new Subscription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import ScoringEndpointRequest\n",
    "\n",
    "print(asset_deployment_details['metadata']['url'])\n",
    "subscription = ai_client.subscriptions.add(\n",
    "        background_mode=False,\n",
    "        data_mart_id=data_mart_id,\n",
    "        service_provider_id=service_provider_id,\n",
    "        asset=Asset(\n",
    "            asset_id=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"asset_id\"],\n",
    "            name=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"name\"],\n",
    "            url=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"url\"],\n",
    "            asset_type=AssetTypes.MODEL,\n",
    "            input_data_type=InputDataType.STRUCTURED,\n",
    "            problem_type=ProblemType.BINARY_CLASSIFICATION\n",
    "        ),\n",
    "        deployment=AssetDeploymentRequest(\n",
    "            deployment_id=asset_deployment_details['metadata']['guid'],\n",
    "            name=asset_deployment_details['entity']['name'],\n",
    "            deployment_type= DeploymentTypes.ONLINE,\n",
    "            url=asset_deployment_details['metadata']['url'],\n",
    "            scoring_endpoint=ScoringEndpointRequest(url=scoring_endpoint) # score model without shadow deployment\n",
    "        ),\n",
    "        asset_properties=AssetPropertiesRequest(\n",
    "            label_column='CallDrop_prec',\n",
    "            probability_fields=['prediction_probability'],\n",
    "            prediction_field='prediction',\n",
    "            feature_fields = [\"Start_Time_MM_DD_YYYY\",\"Start_Time_HH_MM_SS_s\",\"Weather\",\"Traffic\",\"Total_Calls\",\"Call_Dropped\"],\n",
    "            categorical_fields = [\"Start_Time_MM_DD_YYYY\",\"Start_Time_HH_MM_SS_s\",\"Weather\",\"Traffic\"]\n",
    "        )\n",
    "    ).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_uid = subscription.metadata.id\n",
    "subscription_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(5)\n",
    "payload_data_set_id = None\n",
    "payload_data_set_id = ai_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n",
    "                                                target_target_id=subscription_uid, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
    "if payload_data_set_id is None:\n",
    "    print(\"Payload data set not found. Please check subscription status.\")\n",
    "else:\n",
    "    print(\"Payload data set id: \", payload_data_set_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client.data_sets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client.subscriptions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Perform Inital Payload Logging\n",
    "Note: You may re-use this code snippet by modifying the request_data variable to perform payload logging after finishing the initial dashboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=list(X_test.columns)\n",
    "\n",
    "request_data = {\n",
    "    \"fields\": fields,\n",
    "    \"values\": scoring_data\n",
    "  }\n",
    "request_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "request_data - input to scoring endpoint in supported by Watson OpenScale format\n",
    "response_data - output from scored model in supported by Watson OpenScale format\n",
    "response_time - scoring request response time [ms] (integer type)\n",
    "\n",
    "Example:\n",
    "\n",
    "request_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026]]\n",
    "  }\n",
    "\n",
    "response_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\", \"probability\", \"prediction\", \"DRUG\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026, [0.82, 0.07, 0.0, 0.05, 0.03], 0.0, \"drugY\"]]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "payload_scoring = {\"input_data\": [request_data]}\n",
    "\n",
    "predictions = ml_client.deployments.score(deployment_uid, payload_scoring)\n",
    "\n",
    "print(\"Single record scoring result:\", \"\\n fields:\", predictions[\"predictions\"][0][\"fields\"], \"\\n values: \", predictions[\"predictions\"][0][\"values\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if WML payload logging worked else manually store payload records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from ibm_watson_openscale.supporting_classes.payload_record import PayloadRecord\n",
    "time.sleep(5)\n",
    "pl_records_count = ai_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))\n",
    "if pl_records_count == 0:\n",
    "    print(\"Payload logging did not happen, performing explicit payload logging.\")\n",
    "    ai_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=[PayloadRecord(\n",
    "                   scoring_id=str(uuid.uuid4()),\n",
    "                   request=payload_scoring,\n",
    "                   response={\"fields\": predictions['predictions'][0]['fields'], \"values\":predictions['predictions'][0]['values']},\n",
    "                   response_time=460\n",
    "               )],background_mode=False)\n",
    "    time.sleep(5)\n",
    "    pl_records_count = ai_client.data_sets.get_records_count(payload_data_set_id)\n",
    "    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Setup the Fairness Monitors\n",
    "\n",
    "The code below configures fairness monitoring for our model. It turns on monitoring for two features, _conds(Weather Condition) and Traffic for the cell tower. In each case, we must specify:\n",
    "  * Which model feature to monitor\n",
    "  * One or more **majority** groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n",
    "  * One or more **minority** groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n",
    "  * The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 95%)\n",
    "\n",
    "Additionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 5 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Target(\n",
    "    target_type=TargetTypes.SUBSCRIPTION,\n",
    "    target_id=subscription_uid\n",
    "\n",
    ")\n",
    "parameters = {\n",
    "    \"features\": [\n",
    "        {\"feature\": \"Traffic\",\n",
    "         \"majority\": ['Low'],\n",
    "         \"minority\": ['High','Medium'],\n",
    "         \"threshold\": 0.80\n",
    "         },\n",
    "        {\n",
    "         \"feature\": \"Weather\",\n",
    "         \"majority\": ['Clear'],\n",
    "         \"minority\": ['Haze','Rain','Fog','Partly Cloudy'],\n",
    "         \"threshold\": 0.80\n",
    "         }\n",
    "    ],\n",
    "    \"favourable_class\": [0],\n",
    "    \"unfavourable_class\": [1],\n",
    "    \"min_records\": 50\n",
    "}\n",
    "\n",
    "fairness_monitor_details = ai_client.monitor_instances.create(\n",
    "    data_mart_id=data_mart_id,\n",
    "    background_mode=False,\n",
    "    monitor_definition_id=ai_client.monitor_definitions.MONITORS.FAIRNESS.ID,\n",
    "    target=target,\n",
    "    parameters=parameters).result\n",
    "fairness_monitor_instance_id =fairness_monitor_details.metadata.id\n",
    "fairness_monitor_instance_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some more Payload (Optional for populating your dashboard)\n",
    "\n",
    "If you wish to add some Payload Data. Take different sections of your test dataset and send to OpenScale as shown below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=X_test.head(50)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\n",
    "scoring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=list(X_test.columns)\n",
    "print(len(fields))\n",
    "fields, scoring_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    \"fields\": fields,\n",
    "    \"values\": scoring_data\n",
    "  }\n",
    "request_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "request_data - input to scoring endpoint in supported by Watson OpenScale format\n",
    "response_data - output from scored model in supported by Watson OpenScale format\n",
    "response_time - scoring request response time [ms] (integer type)\n",
    "\n",
    "Example:\n",
    "\n",
    "request_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026]]\n",
    "  }\n",
    "\n",
    "response_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\", \"probability\", \"prediction\", \"DRUG\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026, [0.82, 0.07, 0.0, 0.05, 0.03], 0.0, \"drugY\"]]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "payload_scoring = {\"input_data\": [request_data]}\n",
    "\n",
    "predictions = ml_client.deployments.score(deployment_uid, payload_scoring)\n",
    "\n",
    "print(\"Single record scoring result:\", \"\\n fields:\", predictions[\"predictions\"][0][\"fields\"], \"\\n values: \", predictions[\"predictions\"][0][\"values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "pl_records_count = ai_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))\n",
    "if pl_records_count == 20:\n",
    "    print(\"Payload logging did not happen, performing explicit payload logging.\")\n",
    "    ai_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=[PayloadRecord(\n",
    "                   scoring_id=str(uuid.uuid4()),\n",
    "                   request=payload_scoring,\n",
    "                   response={\"fields\": predictions['predictions'][0]['fields'], \"values\":predictions['predictions'][0]['values']},\n",
    "                   response_time=460\n",
    "               )],background_mode=False)\n",
    "    time.sleep(5)\n",
    "    pl_records_count = ai_client.data_sets.get_records_count(payload_data_set_id)\n",
    "    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
